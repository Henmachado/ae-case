{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "66d3b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pyspark.sql import Column as PysparkColumn\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from typing import List\n",
    "from typing import NamedTuple\n",
    "from typing import Union\n",
    "from typing import Tuple\n",
    "from typing import Optional\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "#consider replace this to your own spark cluster session instead of using local\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "0ff934b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utils functions in order to load the data\n",
    "\n",
    "def read_csv(path: str) -> DataFrame:\n",
    "    '''loads csv data infering the schema'''\n",
    "    \n",
    "    return (\n",
    "        spark.read.format(\"com.databricks.spark.csv\") \n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"treatEmptyValuesAsNulls\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"Tables/{path}\")        \n",
    "    )\n",
    "\n",
    "\n",
    "def create_catalog(tables: List) -> None:\n",
    "    '''creates a local spark-warehouse'''\n",
    "    \n",
    "    for table in tables:\n",
    "        _df = read_csv(table)\n",
    "        _df.createOrReplaceTempView(table)\n",
    "        \n",
    "        \n",
    "def read_json_from_file(file_path: str) -> List:\n",
    "    '''reads the raw json'''\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = f.read()\n",
    "        data = json.loads(json_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_json_to_file(data: List[str], file_path) -> None:\n",
    "    '''write the json back as one object per line'''\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in data:\n",
    "            json.dump(item, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "            \n",
    "def read_json_output(output_file: str, schema: str) -> DataFrame:\n",
    "    '''reads the processed json on spark'''\n",
    "    \n",
    "    return spark.read.schema(schema).json(output_file)\n",
    "\n",
    "\n",
    "def unnest_json_df(df: DataFrame) -> DataFrame:\n",
    "    '''unnest the raw data into columns'''\n",
    "    \n",
    "    return (\n",
    "        df.select(\"account_id\", F.explode(\"transactions\").alias(\"transaction\"))\n",
    "        .select(\n",
    "            F.col(\"transaction.transaction_id\").cast('long').alias(\"id\"),\n",
    "            F.col(\"account_id\").cast('long').alias(\"account_id\"),\n",
    "            F.col(\"transaction.amount\").cast('double').alias(\"amount\"),\n",
    "            F.col(\"transaction.investment_requested_at\").cast('int').alias(\"investment_requested_at\"),\n",
    "            F.col(\"transaction.investment_completed_at\").cast('string').alias(\"investment_completed_at\"),\n",
    "            F.col(\"transaction.status\").cast('string').alias(\"status\"),\n",
    "            F.col(\"transaction.type\").cast('string').alias(\"type\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_investments_df() -> None:\n",
    "    '''data pipeline that consumes the raw json and write it in the catalog'''\n",
    "    \n",
    "    input_file_path = 'Tables/investments/investments_json.txt'\n",
    "    output_file_path = 'Tables/investments/investments.json'\n",
    "    \n",
    "    schema = \"\"\"\n",
    "        account_id STRING,\n",
    "        transactions ARRAY<STRUCT<\n",
    "            transaction_id: STRING,\n",
    "            status: STRING,\n",
    "            amount: STRING,\n",
    "            investment_requested_at: STRING,\n",
    "            investment_completed_at: STRING,\n",
    "            investment_completed_at_timestamp: DATE,\n",
    "            type: STRING\n",
    "        >>\n",
    "    \"\"\"\n",
    "    \n",
    "    json_data = read_json_from_file(input_file_path)\n",
    "    write_json_to_file(json_data, output_file_path)\n",
    "    \n",
    "    json_df = read_json_output(output_file_path,schema)\n",
    "    unnested_df = unnest_json_df(json_df)\n",
    "    \n",
    "    return unnested_df.createOrReplaceTempView(\"investments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "2dbe0a60",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/henrique/PycharmProjects/ae-case/Tables/d_year",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [448], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_catalog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTables/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m create_investments_df()\n",
      "Cell \u001b[0;32mIn [447], line 19\u001b[0m, in \u001b[0;36mcreate_catalog\u001b[0;34m(tables)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m'''creates a local spark-warehouse'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[0;32m---> 19\u001b[0m     _df \u001b[38;5;241m=\u001b[39m \u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     _df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(table)\n",
      "Cell \u001b[0;32mIn [447], line 7\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124;03m'''loads csv data infering the schema'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.databricks.spark.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtreatEmptyValuesAsNulls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTables/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     12\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py:177\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/henrique/PycharmProjects/ae-case/Tables/d_year"
     ]
    }
   ],
   "source": [
    "create_catalog(os.listdir(\"Tables/\"))\n",
    "create_investments_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc964d1",
   "metadata": {},
   "source": [
    "# Problem Statement 1:\n",
    "Your colleague Jane Hopper, the Business Analyst in charge of analyzing customer behavior, who directly\n",
    "consumes data from the Data Warehouse Environment, needs to get all the account's monthly balances between\n",
    "Jan/2020 and Dec/2020. She wasn't able to do it alone, and asked for your help. Add to your solution the SQL\n",
    "query (.sql file) used to retrieve the data needed (the necessary tables were sent in csv format along with this pdf,\n",
    "on folder tables/). Feel free to use the dialect of your choice, but please specify the SQL engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4102b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TRANSFER_IN = \"SUM(CASE WHEN in_or_out LIKE '%in%' THEN amount ELSE 0 END)\"\n",
    "TOTAL_TRANSFER_OUT = \"SUM(CASE WHEN in_or_out LIKE '%out%' THEN amount ELSE 0 END)\"\n",
    "\n",
    "df_account_monthly_balance = spark.sql(f\"\"\"\n",
    "\n",
    "WITH\n",
    "\n",
    "raw_data AS (\n",
    "-- Gets raw data from both pix and non-pix transactions\n",
    "\n",
    "    SELECT *, \"transfer_in\" AS in_or_out, \"non_pix\" AS type\n",
    "    FROM transfer_ins\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT *, \"transfer_out\" AS in_or_out, \"non_pix\" AS type\n",
    "    FROM transfer_outs\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT *, \"pix\" AS type\n",
    "    FROM pix_movements\n",
    "    \n",
    "),\n",
    "\n",
    "report AS (\n",
    "-- Gets 2020's Account Monthly Balance for Pix \n",
    "\n",
    "    SELECT\n",
    "        month(from_unixtime(transaction_requested_at)) AS Month,\n",
    "        account_id                                     AS Customer,\n",
    "        {TOTAL_TRANSFER_IN}                            AS TotalTransferIn,\n",
    "        {TOTAL_TRANSFER_OUT}                           AS TotalTransferOut,\n",
    "        {TOTAL_TRANSFER_IN} - {TOTAL_TRANSFER_OUT}     AS AccountMonthlyBalance    \n",
    "\n",
    "    FROM raw_data\n",
    "   WHERE from_unixtime(transaction_requested_at) >= \"2020-01-01\"\n",
    "     AND from_unixtime(transaction_requested_at) <  \"2021-01-01\"\n",
    "     AND status = \"completed\"\n",
    "    GROUP BY 1,2\n",
    "    \n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM report\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "df_account_monthly_balance.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd3a43",
   "metadata": {},
   "source": [
    "# Problem Statement 2:\n",
    "Data model modification proposal with a visual representation and trade-off analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProductDailyEvents:\n",
    "    \"\"\"\n",
    "    \n",
    "    This class sits between our original source tables and the reporting enviroment. It\n",
    "    transforms data from service tables into a standardized table that will help us to\n",
    "    create more flexible and scalable datamarts\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    table: DataFrame\n",
    "    account_id_col: str\n",
    "    amount_col: str\n",
    "    requested_at_col: str\n",
    "    completed_at_col: str\n",
    "    status_col: str\n",
    "    in_or_out_col: str\n",
    "    product: str\n",
    "        \n",
    "    accounts: DataFrame = spark.table(\"accounts\").select(\"account_id\",\"customer_id\")\n",
    "    customers: DataFrame = spark.table(\"customers\").select(\"country_name\",\"customer_id\")\n",
    "        \n",
    "    def get_transformed_table(self) -> DataFrame:\n",
    "        return (\n",
    "            self.table\n",
    "            .select(\n",
    "                F.col(self.account_id_col).alias(\"account_id\"),\n",
    "                F.col(self.amount_col).alias(\"amount\"),\n",
    "                F.col(self.requested_at_col).alias(\"txn_requested_at\"),\n",
    "                F.col(self.completed_at_col).alias(\"txn_completed_at\"),\n",
    "                F.col(self.status_col).alias(\"status\"),\n",
    "                F.when(\n",
    "                    F.col(self.in_or_out_col).like(\"%_in%\"),F.lit(\"in\")\n",
    "                ).otherwise(F.lit(\"out\")).alias(\"in_or_out\"),\n",
    "                F.lit(self.product).alias(\"product\"),\n",
    "            )\n",
    "            .join(self.accounts, on=\"account_id\", how=\"left\")\n",
    "            .join(self.customers, on=\"customer_id\", how=\"left\")\n",
    "            .withColumn(\"day\",F.dayofmonth(F.from_unixtime(\"txn_requested_at\")))\n",
    "            .withColumn(\"month\",F.month(F.from_unixtime(\"txn_requested_at\")))\n",
    "            .withColumn(\"event_date\",F.date_format(F.from_unixtime(\"txn_requested_at\"),\"yyyy-MM-dd\"))\n",
    "            .drop(\"customer_id\")\n",
    "        )\n",
    "    \n",
    "    def write_in_spark_warehouse(self,mode) -> None:\n",
    "        df = self.get_transformed_table()\n",
    "        df.write.mode(mode).format(\"parquet\").saveAsTable(\"products_daily_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_movements = spark.table(\"pix_movements\")\n",
    "transfer_ins = spark.table(\"transfer_ins\").withColumn(\"in_or_out\", F.lit(\"non_pix_in\"))\n",
    "transfer_outs = spark.table(\"transfer_outs\").withColumn(\"in_or_out\", F.lit(\"non_pix_out\"))\n",
    "investments = spark.table(\"investments\")\n",
    "\n",
    "daily_events_pix = ProductDailyEvents(\n",
    "    table=pix_movements,\n",
    "    account_id_col=\"account_id\",\n",
    "    amount_col=\"pix_amount\",\n",
    "    requested_at_col=\"pix_requested_at\",\n",
    "    completed_at_col=\"pix_completed_at\",\n",
    "    status_col=\"status\",\n",
    "    in_or_out_col=\"in_or_out\",\n",
    "    product=\"pix\"\n",
    ")\n",
    "daily_events_pix.write_in_spark_warehouse(mode=\"overwrite\")\n",
    "\n",
    "daily_events_transfer_in = ProductDailyEvents(\n",
    "    table=transfer_ins,\n",
    "    account_id_col=\"account_id\",\n",
    "    amount_col=\"amount\",\n",
    "    requested_at_col=\"transaction_requested_at\",\n",
    "    completed_at_col=\"transaction_completed_at\",\n",
    "    status_col=\"status\",\n",
    "    in_or_out_col=\"in_or_out\",\n",
    "    product=\"non_pix\",\n",
    ")\n",
    "daily_events_transfer_in.write_in_spark_warehouse(mode=\"append\")\n",
    "\n",
    "daily_events_transfer_out = ProductDailyEvents(\n",
    "    table=transfer_outs,\n",
    "    account_id_col=\"account_id\",\n",
    "    amount_col=\"amount\",\n",
    "    requested_at_col=\"transaction_requested_at\",\n",
    "    completed_at_col=\"transaction_completed_at\",\n",
    "    status_col=\"status\",\n",
    "    in_or_out_col=\"in_or_out\",\n",
    "    product=\"non_pix\",\n",
    ")\n",
    "daily_events_transfer_out.write_in_spark_warehouse(mode=\"append\")\n",
    "\n",
    "daily_events_investments = ProductDailyEvents(\n",
    "    table=investments,\n",
    "    account_id_col=\"account_id\",\n",
    "    amount_col=\"amount\",\n",
    "    requested_at_col=\"investment_requested_at\",\n",
    "    completed_at_col=\"investment_completed_at\",\n",
    "    status_col=\"status\",\n",
    "    in_or_out_col=\"type\",\n",
    "    product=\"investments\",\n",
    ")\n",
    "daily_events_investments.write_in_spark_warehouse(mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"products_daily_events\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1a19a",
   "metadata": {},
   "source": [
    "# Problem Statement 3:\n",
    "Migration plan and strategy in order to implement the data model modification\n",
    "proposal mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c9407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd853bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b2b1d95",
   "metadata": {},
   "source": [
    "# Problem Statement 4:\n",
    "On another note, Jane's friend, Pepino, wants to know how well our PIX product is doing inside Nubank. For that,\n",
    "he wants your help to come up with indicators that can be used to track the technical and business performance of\n",
    "the product. Which metrics would you suggest to track and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6319d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDimensions(NamedTuple):\n",
    "    \"\"\"\n",
    "    Defines all custom dimensions that can be add to the 'products_daily_events' table in\n",
    "    order to create our businness metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    WINDOW_ACCOUNT = (\n",
    "        Window()\n",
    "        .partitionBy(\"account_id\")\n",
    "        .orderBy(F.col(\"txn_requested_at\"))\n",
    "    )\n",
    "    \n",
    "    WINDOW_ACCOUNT_24H = (\n",
    "        Window()\n",
    "        .partitionBy(\"account_id\")\n",
    "        .orderBy(F.col(\"txn_requested_at\").cast(\"long\"))\n",
    "        .rangeBetween(-24, Window.currentRow)\n",
    "    )\n",
    "    \n",
    "    WINDOW_ACCOUNT_EVER = (\n",
    "        Window()\n",
    "        .partitionBy(\"account_id\")\n",
    "        .orderBy(F.col(\"txn_requested_at\"))\n",
    "        .rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    )\n",
    "    \n",
    "    TXN_REQUESTED_AT_TS = (\n",
    "        F.from_unixtime(\"txn_requested_at\")\n",
    "    ).alias(\"txn_requested_at_ts\")\n",
    "    \n",
    "    TXN_COMPLETED_AT_TS = (\n",
    "        F.from_unixtime(\"txn_completed_at\")\n",
    "    ).alias(\"txn_completed_at_ts\")\n",
    "    \n",
    "    TIME_TO_COMPLETE_IN_SECONDS = (\n",
    "        F.col(\"txn_completed_at\").cast(\"long\")-F.col(\"txn_requested_at\").cast(\"long\")\n",
    "    ).alias(\"time_to_complete_in_seconds\")\n",
    "    \n",
    "    COUNT_TXNS_PER_ACCOUNT_EVER = (\n",
    "        F.sum(F.lit(1)).over(WINDOW_ACCOUNT_EVER)\n",
    "    ).alias(\"count_txns_orders_per_account_ever\")\n",
    "    \n",
    "    LAST_TXN_DATE = (\n",
    "        F.lag(\"txn_requested_at\",1).over(WINDOW_ACCOUNT)\n",
    "    ).alias(\"last_txn_date\")\n",
    "    \n",
    "    TIME_FROM_LAST_TXN_IN_DAYS = (\n",
    "        ((F.col(\"txn_completed_at\").cast(\"long\")-LAST_TXN_DATE.cast(\"long\"))/86400)\n",
    "    ).alias(\"time_from_last_txn_in_days\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all(cls) -> List[F.Column]:\n",
    "        \"\"\" \n",
    "        The get_all() class method is an option to generate all at once in a single list \n",
    "        \"\"\"\n",
    "        return [\n",
    "            cls.TXN_REQUESTED_AT_TS,\n",
    "            cls.TXN_COMPLETED_AT_TS,\n",
    "            cls.TIME_TO_COMPLETE_IN_SECONDS,\n",
    "            cls.COUNT_TXNS_PER_ACCOUNT_EVER,\n",
    "            cls.LAST_TXN_DATE,\n",
    "            cls.TIME_FROM_LAST_TXN_IN_DAYS,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b52b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(NamedTuple):\n",
    "    \"\"\"\n",
    "    Defines all logics and businnes rules for our metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    FAILED = (F.col(\"status\")==\"failed\")\n",
    "    \n",
    "    COMPLETED = (F.col(\"status\")==\"completed\")\n",
    "    \n",
    "    FIRST_TXN = (F.col(\"count_txns_orders_per_account_ever\")==1)\n",
    "    \n",
    "    TOTAL_AMOUNT = F.sum(F.col(\"amount\")).alias(\"total_amount\")\n",
    "    \n",
    "    TOTAL_AMOUNT_COMPLETED = (\n",
    "        F.sum(F.when(COMPLETED,F.col(\"amount\")).otherwise(0))\n",
    "    ).alias(\"total_amount_completed\")\n",
    "    \n",
    "    TOTAL_AMOUNT_FAILED = (\n",
    "        F.sum(F.when(FAILED,F.col(\"amount\")).otherwise(0))\n",
    "    ).alias(\"total_amount_failed\")\n",
    "    \n",
    "    TOTAL_TRANSFER_IN = (\n",
    "        F.sum(F.when(F.col(\"in_or_out\")==\"in\",F.col(\"amount\")).otherwise(0))\n",
    "    ).alias(\"total_transfer_in\")\n",
    "\n",
    "    TOTAL_TRANSFER_OUT = (\n",
    "        F.sum(F.when(F.col(\"in_or_out\")==\"out\",F.col(\"amount\")).otherwise(0))\n",
    "    ).alias(\"total_transfer_out\")\n",
    "    \n",
    "    ACCOUNT_MONTHLY_BALANCE =  (\n",
    "        TOTAL_TRANSFER_IN - TOTAL_TRANSFER_OUT\n",
    "    ).alias(\"account_monthly_balance\")\n",
    "    \n",
    "    FAIL_RATE = F.round(\n",
    "        TOTAL_AMOUNT_FAILED / TOTAL_AMOUNT_COMPLETED, 2\n",
    "    ).alias(\"fail_rate\")\n",
    "    \n",
    "    COMPLETED_RATE = F.round(\n",
    "        TOTAL_AMOUNT_COMPLETED / TOTAL_AMOUNT, 2\n",
    "    ).alias(\"completed_rate\")\n",
    "    \n",
    "    AVG_TIME_TO_COMPLETED = (\n",
    "        F.avg(F.col(\"time_to_complete_in_seconds\"))\n",
    "    ).alias(\"avg_time_to_completed\") \n",
    "    \n",
    "    AVG_RECENCY = (\n",
    "        F.avg(F.col(\"time_from_last_txn_in_days\"))\n",
    "    ).alias(\"avg_recency\")\n",
    "    \n",
    "    TOTAL_ORDERS = (\n",
    "        F.sum(F.lit(1))\n",
    "    ).alias(\"total_orders\")\n",
    "    \n",
    "    TOTAL_FIRST_TXN_ORDERS = (\n",
    "        F.sum(F.when(FIRST_TXN,F.lit(1)).otherwise(0))\n",
    "    ).alias(\"total_first_txn_orders\")\n",
    "    \n",
    "    ATV = (\n",
    "        F.avg(F.col(\"amount\"))\n",
    "    ).alias(\"atv\")\n",
    "    \n",
    "    P99_TIME_TO_COMPLETED = (\n",
    "        F.percentile_approx(\"time_to_complete_in_seconds\", 0.99).alias(\"p99\")\n",
    "    )\n",
    "    \n",
    "    P90_TIME_TO_COMPLETED = (\n",
    "        F.percentile_approx(\"time_to_complete_in_seconds\", 0.90).alias(\"p90\")\n",
    "    )\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all(cls) -> List[PysparkColumn]:\n",
    "        \"\"\" \n",
    "        The get_all() class method is an option to generate all at once in a single list \n",
    "        \"\"\"\n",
    "        return [\n",
    "            cls.TOTAL_AMOUNT,\n",
    "            cls.TOTAL_AMOUNT_COMPLETED,\n",
    "            cls.TOTAL_AMOUNT_FAILED,\n",
    "            cls.FAIL_RATE,\n",
    "            cls.COMPLETED_RATE,\n",
    "            cls.AVG_TIME_TO_COMPLETED,\n",
    "            cls.AVG_RECENCY,\n",
    "            cls.TOTAL_ORDERS,\n",
    "            cls.TOTAL_FIRST_PIX_ORDERS,\n",
    "            cls.ATV,\n",
    "            cls.P99_TIME_TO_COMPLETED,\n",
    "            cls.P90_TIME_TO_COMPLETED,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_metrics_plot(\n",
    "    df: DataFrame,\n",
    "    metric: List[PysparkColumn],\n",
    "    title: str, \n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Auxiliar function to help plot the monthly metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    return (df\n",
    "            .groupBy(\"month\")\n",
    "            .agg(*metric)\n",
    "            .sort(F.asc(\"month\"))\n",
    "            .toPandas()\n",
    "            .plot\n",
    "            .line(x=\"month\", title=title)\n",
    "            .legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz = (spark.table(\"products_daily_events\")\n",
    "          .where(F.col(\"product\")==\"pix\")\n",
    "          .select(\"*\", *CustomDimensions.get_all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_monthly_metrics_plot(df_viz,[Metrics.TOTAL_FIRST_TXN_ORDERS],\"Total First Pix Orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_monthly_metrics_plot(df_viz,[Metrics.FAIL_RATE],\"Pix Fail Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "     Metrics.AVG_TIME_TO_COMPLETED,\n",
    "     Metrics.P99_TIME_TO_COMPLETED,\n",
    "     Metrics.P90_TIME_TO_COMPLETED,\n",
    "]\n",
    "\n",
    "get_monthly_metrics_plot(df_viz,metrics,\"Time to Completed Metrics (in seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_monthly_metrics_plot(df_viz,[Metrics.ATV],\"Pix ATV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6fa23c",
   "metadata": {},
   "source": [
    "# Problem Statement 5:\n",
    "Another business analyst, Sophia, would like your help to analyze how much money Nubank's customers' have on their investment account on a daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvestmentsMetrics(NamedTuple):\n",
    "    \"\"\"\n",
    "    Defines all logics, businnes rules and constants for our investments metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    INVESTMENTS_LAG_WINDOW = (\n",
    "        Window()\n",
    "        .partitionBy(\"account_id\")\n",
    "        .orderBy(F.col(\"event_date\"))\n",
    "    )\n",
    "\n",
    "    INVESTMENTS_CUM_WINDOW = (\n",
    "        Window()\n",
    "        .partitionBy(\"account_id\")\n",
    "        .orderBy(F.col(\"event_date\"))\n",
    "        .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "    )\n",
    "\n",
    "    FIXED_RATE = 0.0001\n",
    "\n",
    "    TOTAL_DEPOSIT = (\n",
    "        F.sum(F.when(F.col(\"in_or_out\")==\"in\",F.col(\"amount\")).otherwise(0))\n",
    "    ).alias(\"deposit\")\n",
    "\n",
    "    TOTAL_WITHDRAWAL = (\n",
    "        F.sum(F.when(F.col(\"in_or_out\")==\"out\",F.col(\"amount\")).otherwise(0))\n",
    "    ).alias(\"withdrawal\")\n",
    "\n",
    "    TOTAL_BALANCE = (\n",
    "        TOTAL_DEPOSIT-TOTAL_WITHDRAWAL\n",
    "    ).alias(\"balance\")\n",
    "\n",
    "    PREVIOUS_DAY_TOTAL_BALANCE = (\n",
    "        F.lag(TOTAL_BALANCE,default=0).over(INVESTMENTS_LAG_WINDOW)\n",
    "    ).alias(\"previous_day_balance\")\n",
    "\n",
    "    TOTAL_MOVEMENTS = (\n",
    "        F.sum(TOTAL_BALANCE).over(INVESTMENTS_CUM_WINDOW)\n",
    "    ).alias(\"movements\")\n",
    "\n",
    "    _END_OF_DAY_INCOME = (\n",
    "        TOTAL_MOVEMENTS*F.lit(FIXED_RATE)\n",
    "    ).alias(\"_end_of_day_income\")\n",
    "    \n",
    "    END_OF_DAY_INCOME = (\n",
    "        F.when(_END_OF_DAY_INCOME<0,F.lit(0)).otherwise(_END_OF_DAY_INCOME)\n",
    "    ).alias(\"end_of_day_income\")\n",
    "\n",
    "    ACCOUNT_DAILY_BALANCE = (\n",
    "        TOTAL_MOVEMENTS+END_OF_DAY_INCOME\n",
    "    ).alias(\"account_daily_balance\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all(cls) -> List[PysparkColumn]:\n",
    "        \"\"\" \n",
    "        The get_all() class method is an option to generate all at once in a single list \n",
    "        \"\"\"\n",
    "        return [\n",
    "            cls.TOTAL_DEPOSIT,\n",
    "            cls.TOTAL_WITHDRAWAL,\n",
    "            cls.TOTAL_BALANCE,\n",
    "            cls.PREVIOUS_DAY_TOTAL_BALANCE,\n",
    "            cls.TOTAL_MOVEMENTS,\n",
    "            cls.END_OF_DAY_INCOME,\n",
    "            cls.ACCOUNT_DAILY_BALANCE,\n",
    "        ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fb36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_investments_daily_balance_df(\n",
    "    df: DataFrame,\n",
    "    interval: Tuple[str,str],\n",
    "    metrics: InvestmentsMetrics = InvestmentsMetrics(),\n",
    ") -> DataFrame:\n",
    "    ''' Creates the Investments Daily Balance Report '''\n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        .where(F.col(\"event_date\").between(*interval))\n",
    "        .where(F.col(\"status\")==\"completed\")\n",
    "        .where(F.col(\"product\")==\"investments\")\n",
    "        .groupBy(\"event_date\",\"day\",\"month\",\"account_id\")\n",
    "        .agg(*metrics.get_all())\n",
    "        .drop(\"event_date\",\"balance\",\"previous_day_balance\",\"movements\")\n",
    "    )\n",
    "\n",
    "def create_account_monthly_balance(\n",
    "    df: DataFrame, \n",
    "    interval: Tuple[str,str],\n",
    "    products: List[str],\n",
    "    metrics: Metrics = Metrics(),\n",
    ") -> DataFrame:\n",
    "    ''' Creates the Account Monthly Balance Report '''\n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        .where(F.col(\"event_date\").between(*interval))\n",
    "        .where(F.col(\"status\")==\"completed\")\n",
    "        .where(F.col(\"product\").isin(products))\n",
    "        .groupBy(\"month\",\"account_id\")\n",
    "        .agg(metrics.TOTAL_TRANSFER_IN,\n",
    "             metrics.TOTAL_TRANSFER_OUT,\n",
    "             metrics.ACCOUNT_MONTHLY_BALANCE,\n",
    "            )\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_daily_events = spark.table(\"products_daily_events\")\n",
    "\n",
    "create_investments_daily_balance_df(\n",
    "    df=df_products_daily_events,\n",
    "    interval=[\"2020-01-01\",\"2021-01-01\"],\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_account_monthly_balance(\n",
    "    df=df_products_daily_events,\n",
    "    interval=[\"2020-01-01\",\"2021-01-01\"],\n",
    "    products=[\"non_pix\",\"pix\"]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d885deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
